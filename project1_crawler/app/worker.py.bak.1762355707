#!/usr/bin/env python3
import os
from pathlib import Path

# --- force-load envs BEFORE importing config ---
try:
    from dotenv import load_dotenv
except ImportError:
    load_dotenv = None

# order matters: /etc/... first, then local .env
env_paths = [
    Path("/etc/social-pipeline.env"),
    Path("/home/irajmohan/social-pipeline/.env"),
    Path(__file__).resolve().parent.parent / ".env",
]
if load_dotenv is not None:
    for p in env_paths:
        if p.exists():
            load_dotenv(p, override=True)

# now import the real config (this will now see your NEW BSKY_ACTORS)
from datetime import datetime, timezone, timedelta
from typing import Dict, List

from faktory import Worker
from logutil import get_logger
from config import (
    DATABASE_URL,
    FAKTORY_URL,
    CHAN_LAST_SEEN_PATH,
    BSKY_CURSORS_PATH,
    BSKY_HANDLE,
    BSKY_APP_PASSWORD,
    BSKY_ACTORS,
    BSKY_HEAD_PAGES,
    BSKY_BACKFILL_PAGES,
    BSKY_MAX_BACKFILL_HOURS,
)
from state import load_json, save_json
from db import get_conn, insert_4chan_posts, insert_bsky_posts
from chan_client import get_catalog, get_thread
from bsky_client_cached import get_bsky_client
from bsky_client import get_author_feed, as_primitive

try:
    from atproto_client.exceptions import RequestException
except ImportError:
    RequestException = Exception

logger = get_logger("worker")

# make sure faktory URL is visible to faktory lib
if FAKTORY_URL and not os.getenv("FAKTORY_URL"):
    os.environ["FAKTORY_URL"] = FAKTORY_URL

def safe_get_author_feed(client, actor, cursor):
    # tolerate vanished or invalid actors across atproto_client versions
    from atproto_client import exceptions as at_ex
    try:
        return get_author_feed(client, actor, cursor)
    except at_ex.BadRequestError as e:
        msg = str(e)
        # map common 4xx text variants to "profile-not-found"
        if ("Profile not found" in msg
            or "InvalidRequest" in msg
            or "NotFound" in msg
            or "Record not found" in msg):
            print(f"[bsky] skip actor={actor} reason=profile-not-found", flush=True)
            return None, None
        raise
    except Exception as e:
        # Older clients may not expose NotFoundError symbol; fall back by name
        name = e.__class__.__name__
        if name in ("NotFoundError", "RecordNotFoundError"):
            print(f"[bsky] skip actor={actor} reason=not-found", flush=True)
            return None, None
        raise

def crawl_board(board: str):
    logger.info(f"4chan: crawl board={board}")

    last_seen_all = load_json(CHAN_LAST_SEEN_PATH)
    board_map: Dict[str, int] = {k: int(v) for k, v in last_seen_all.get(board, {}).items()}

    try:
        catalog = get_catalog(board)
    except Exception as e:
        logger.warning(f"4chan: catalog failed board={board}: {e}")
        catalog = []

    active_threads: List[int] = []
    for page in catalog:
        for t in page.get("threads", []):
            if "no" in t:
                active_threads.append(int(t["no"]))

    inserted = 0
    conn = get_conn(DATABASE_URL)
    try:
        for thread_no in active_threads:
            last_seen = int(board_map.get(str(thread_no), 0))
            try:
                tjson = get_thread(board, thread_no)
            except Exception:
                continue

            posts = tjson.get("posts", [])
            new_posts = [p for p in posts if int(p.get("no", 0)) > last_seen]
            if not new_posts:
                continue

            row_batch = []
            for p in new_posts:
                post_no = int(p.get("no", 0))
                ts = int(p.get("time", 0))
                created_at = datetime.fromtimestamp(ts, tz=timezone.utc)
                has_media = any(k in p for k in ("filename", "ext", "tim"))

                row_batch.append({
                    "board_name": board,
                    "thread_number": thread_no,
                    "post_number": post_no,
                    "created_at": created_at,
                    "data": p,
                    "has_media": has_media,
                })

                board_map[str(thread_no)] = post_no

            inserted_now = insert_4chan_posts(conn, row_batch)
            inserted += inserted_now

        last_seen_all[board] = {k: str(v) for k, v in board_map.items()}
        save_json(CHAN_LAST_SEEN_PATH, last_seen_all)
    finally:
        conn.close()

    logger.info(f"4chan: board={board} inserted={inserted}")
    return {"board": board, "inserted": inserted}

# ---------- BLUESKY ----------
def crawl_bsky_actor(actor: str):
    DENY = {"nflmemewar.bsky.social","theathletic.com"}

    if actor in DENY:

        logger.info("[bsky] denylist skip actor=%s", actor)

        return
    logger.info(f"bsky: actor={actor}")

    cursors: Dict[str, str] = load_json(BSKY_CURSORS_PATH)
    cursor = cursors.get(actor)

    client = get_bsky_client(BSKY_HANDLE, BSKY_APP_PASSWORD)
    rows = []
    inserted_total = 0

    # HEAD crawl
    pages_fetched = 0
    next_cursor = None
    while pages_fetched < BSKY_HEAD_PAGES:
        feed, next_cursor = safe_get_author_feed(client, actor, None if pages_fetched == 0 else next_cursor)
        if feed is None:
            # invalid actor, stop trying pages for this actor
            break
        pages_fetched += 1

        for item in feed:
            if getattr(item, "reason", None):
                continue
            post = getattr(item, "post", None)
            if post is None:
                continue

            uri = getattr(post, "uri", None)
            dt = getattr(post, "indexed_at", None)
            data = as_primitive(post)

            like_count = getattr(post, "like_count", None)
            repost_count = getattr(post, "repost_count", None)
            has_media = bool(getattr(post, "embed", None))

            rows.append({
                "actor": actor,
                "uri": uri,
                "created_at": dt,
                "data": data,
                "stance": None,
                "like_count": like_count,
                "repost_count": repost_count,
                "has_media": has_media,
            })

        if not next_cursor:
            break

    # BACKFILL (optional)
    if BSKY_BACKFILL_PAGES > 0 and BSKY_MAX_BACKFILL_HOURS > 0:
        cutoff = datetime.now(timezone.utc) - timedelta(hours=BSKY_MAX_BACKFILL_HOURS)
        pages_fetched = 0
        while pages_fetched < BSKY_BACKFILL_PAGES:
            feed, cursor = safe_get_author_feed(client, actor, cursor)
            pages_fetched += 1

            for item in feed:
                if getattr(item, "reason", None):
                    continue
                post = getattr(item, "post", None)
                if post is None:
                    continue

                raw_dt = getattr(post, "indexed_at", None)
                parsed_dt = None
                if raw_dt:
                    if isinstance(raw_dt, str):
                        ts = raw_dt.replace("Z", "+00:00")
                        try:
                            parsed_dt = datetime.fromisoformat(ts)
                        except Exception:
                            parsed_dt = None
                    else:
                        parsed_dt = raw_dt

                if parsed_dt and parsed_dt < cutoff:
                    pages_fetched = BSKY_BACKFILL_PAGES
                    break

                uri = getattr(post, "uri", None)
                data = as_primitive(post)
                like_count = getattr(post, "like_count", None)
                repost_count = getattr(post, "repost_count", None)
                has_media = bool(getattr(post, "embed", None))

                rows.append({
                    "actor": actor,
                    "uri": uri,
                    "created_at": parsed_dt or raw_dt,
                    "data": data,
                    "stance": None,
                    "like_count": like_count,
                    "repost_count": repost_count,
                    "has_media": has_media,
                })

            if not cursor:
                break

    if rows:
        conn = get_conn(DATABASE_URL)
        try:
            inserted_total = insert_bsky_posts(conn, rows)
        finally:
            conn.close()

    if next_cursor:
        cursors[actor] = next_cursor
        save_json(BSKY_CURSORS_PATH, cursors)

    logger.info(f"bsky: actor={actor} inserted_total={inserted_total}")
    return {"actor": actor, "inserted_total": inserted_total}

def main():
    # IMPORTANT: BSKY_ACTORS is now coming from the env / config we just loaded
    w = Worker(queues=['default', 'crawl'])
    w.register('crawl_board', crawl_board)
    w.register('crawl_bsky_actor', crawl_bsky_actor)
    w.run()

if __name__ == "__main__":
    main()
